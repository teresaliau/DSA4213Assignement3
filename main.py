# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EGXPfWEFprBowC_ZT8ZmUrZ_J3SrJqpJ
"""

import os
import torch
import json
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from transformers import Trainer
import time

# Set device (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Running on device: {device}")

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# Model paths
model_path_full = './models/model_full_finetuned'  # Local path for full fine-tuned model
model_path_lora = './models/model_lora_adapter'  # Local path for LoRA model

# Load models
print(f"Loading full fine-tuned model from {model_path_full}")
model_full = AutoModelForSequenceClassification.from_pretrained(model_path_full, num_labels=6).to(device)

print(f"Loading LoRA model from {model_path_lora}")
model_lora = AutoModelForSequenceClassification.from_pretrained(model_path_lora, num_labels=6).to(device)

# Define label names
label_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

# Load emotion dataset
print("Loading emotion dataset...")
dataset = load_dataset("emotion")

# Tokenize dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Split validation dataset into validation and test
val_test = tokenized_datasets['validation'].train_test_split(test_size=0.5, seed=42)
tokenized_datasets['validation'] = val_test['train']
tokenized_datasets['test'] = val_test['test']

# Prediction function
def predict_emotion(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Full FT prediction
    model_full.eval()
    with torch.no_grad():
        outputs_full = model_full(**inputs)
        probs_full = torch.nn.functional.softmax(outputs_full.logits, dim=-1)[0]
        pred_full = torch.argmax(probs_full).item()

    # LoRA prediction
    model_lora.eval()
    with torch.no_grad():
        outputs_lora = model_lora(**inputs)
        probs_lora = torch.nn.functional.softmax(outputs_lora.logits, dim=-1)[0]
        pred_lora = torch.argmax(probs_lora).item()

    return {
        'text': text,
        'full_ft': label_names[pred_full],
        'full_conf': float(probs_full[pred_full]),
        'lora': label_names[pred_lora],
        'lora_conf': float(probs_lora[pred_lora]),
        'agree': pred_full == pred_lora
    }

# Evaluation metrics computation
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
    f1_macro = f1_metric.compute(predictions=predictions, references=labels, average='macro')
    f1_weighted = f1_metric.compute(predictions=predictions, references=labels, average='weighted')
    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')
    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')

    return {
        'accuracy': accuracy['accuracy'],
        'f1_weighted': f1_weighted['f1'],
        'f1_macro': f1_macro['f1'],
        'precision': precision['precision'],
        'recall': recall['recall']
    }

# Results Comparison and Saving
def compare_models(results_full, results_lora):
    print("Comparing results...")

    comparison_data = {
        "Metric": [
            "Trainable Parameters", "Accuracy", "F1 Score"
        ],
        "Full Fine-tuning": [
            f"{results_full['trainable_params']:,}",
            f"{results_full['eval_accuracy']:.4f}",
            f"{results_full['eval_f1_weighted']:.4f}"
        ],
        "LoRA": [
            f"{results_lora['trainable_params']:,}",
            f"{results_lora['eval_accuracy']:.4f}",
            f"{results_lora['eval_f1_weighted']:.4f}"
        ]
    }

    comparison_df = pd.DataFrame(comparison_data)
    print("\n" + comparison_df.to_string(index=False))

    # Save results to JSON
    results_dict = {
        "full_finetuning": results_full,
        "lora": results_lora
    }
    with open('results_summary.json', 'w') as f:
        json.dump(results_dict, f, indent=2)

    print("\nResults saved to results_summary.json")

# Main function to execute script
if __name__ == "__main__":
    # Check if models are available locally
    if not os.path.exists(model_path_full) or not os.path.exists(model_path_lora):
        print("Models not found locally. Please download them and place them in the 'models' folder.")
        print("Full Fine-tuned model: https://drive.google.com/drive/folders/1exOmAGt4iIYT3tBniyPrJHAzLbUnJLKy")
        print("LoRA Model: https://drive.google.com/drive/folders/1G0PzrUMzjJ4ZNlyt_h_4X5t6X8fY0BK-")
    else:
        print("Models found. Starting evaluation...\n")

        # Evaluate Full Fine-Tuning Model
        print("Evaluating Full Fine-Tuning model...")
        model_full.eval()
        trainer_full = Trainer(
            model=model_full,
            eval_dataset=tokenized_datasets["test"],
            compute_metrics=compute_metrics
        )
        results_full = trainer_full.evaluate()

        # Evaluate LoRA Model
        print("Evaluating LoRA model...")
        model_lora.eval()
        trainer_lora = Trainer(
            model=model_lora,
            eval_dataset=tokenized_datasets["test"],
            compute_metrics=compute_metrics
        )
        results_lora = trainer_lora.evaluate()

        # Compare models' results
        compare_models(results_full, results_lora)