# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ohAgLnMlPj1lfvw9X37UORiqHOGYhQiI
"""

import torch
import json
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel
from datasets import load_dataset
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

import os
if not os.path.exists("./model_full_finetuned"):
    print("\\nERROR: ./model_full_finetuned/ not found!")
    print("Please download from: https://drive.google.com/drive/folders/1exOmAGt4iIYT3tBniyPrJHAzLbUnJLKy")
    exit(1)

if not os.path.exists("./model_lora_adapter"):
    print("\\nERROR: ./model_lora_adapter/ not found!")
    print("Please download from: https://drive.google.com/drive/folders/1G0PzrUMzjJ4ZNlyt_h_4X5t6X8fY0BK-")
    exit(1)

# Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\\nDevice: {device}")

label_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']

# Load tokenizer
print("\\nLoading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# Load models
print("Loading Full Fine-tuned model...")
model_full = AutoModelForSequenceClassification.from_pretrained(
    "./model_full_finetuned", num_labels=6
).to(device)

print("Loading LoRA adapter...")
base_model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=6
).to(device)
model_lora = PeftModel.from_pretrained(base_model, "./model_lora_adapter")

print("\\nModels loaded successfully!")

# Load test data
print("\\nLoading test dataset...")
dataset = load_dataset("emotion")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length",
                    truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
val_test = tokenized_datasets['validation'].train_test_split(test_size=0.5, seed=42)
test_dataset = val_test['test']

print(f"Test examples: {len(test_dataset)}")

# Evaluate function
def evaluate_model(model, dataset, model_name):
    model.eval()
    predictions = []
    true_labels = dataset['label']

    print(f"\\nEvaluating {model_name}...")
    for i in range(len(dataset)):
        inputs = tokenizer(dataset[i]['text'], return_tensors="pt",
                          truncation=True, max_length=128, padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            pred = torch.argmax(outputs.logits, dim=-1).item()
            predictions.append(pred)

    # Calculate metrics
    accuracy = sum(p == t for p, t in zip(predictions, true_labels)) / len(true_labels)

    print(f"  Accuracy: {accuracy:.4f}")

    return predictions, true_labels

# Evaluate both models
preds_full, true_labels = evaluate_model(model_full, test_dataset, "Full Fine-tuning")
preds_lora, _ = evaluate_model(model_lora, test_dataset, "LoRA")

# Generate confusion matrices
print("\\nGenerating visualizations...")
cm_full = confusion_matrix(true_labels, preds_full)
cm_lora = confusion_matrix(true_labels, preds_lora)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_names, yticklabels=label_names, ax=axes[0])
axes[0].set_title('Full Fine-tuning')
axes[0].set_ylabel('True Label')
axes[0].set_xlabel('Predicted Label')

sns.heatmap(cm_lora, annot=True, fmt='d', cmap='Greens',
            xticklabels=label_names, yticklabels=label_names, ax=axes[1])
axes[1].set_title('LoRA')
axes[1].set_ylabel('True Label')
axes[1].set_xlabel('Predicted Label')

plt.tight_layout()
plt.savefig('confusion_matrices_reproduced.png', dpi=300)
print("Saved: confusion_matrices_reproduced.png")

# Classification reports
print("\\n" + "="*70)
print("Full Fine-tuning Classification Report:")
print("="*70)
print(classification_report(true_labels, preds_full, target_names=label_names))

print("\\n" + "="*70)
print("LoRA Classification Report:")
print("="*70)
print(classification_report(true_labels, preds_lora, target_names=label_names))


# Interactive demo
print("\n" + "="*70)
print("Interactive Demo - Test Your Own Sentences!")
print("="*70)
print("(Press Enter without text to exit)\n")

def predict_emotion(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True,
                      max_length=128, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Full FT
    model_full.eval()
    with torch.no_grad():
        outputs_full = model_full(**inputs)
        probs_full = torch.nn.functional.softmax(outputs_full.logits, dim=-1)[0]
        pred_full = torch.argmax(probs_full).item()

    # LoRA
    model_lora.eval()
    with torch.no_grad():
        outputs_lora = model_lora(**inputs)
        probs_lora = torch.nn.functional.softmax(outputs_lora.logits, dim=-1)[0]
        pred_lora = torch.argmax(probs_lora).item()

    print(f"\nText: \"{text}\"")
    print(f"Full FT: {label_names[pred_full].upper()} ({probs_full[pred_full]*100:.1f}%)")
    print(f"LoRA:    {label_names[pred_lora].upper()} ({probs_lora[pred_lora]*100:.1f}%)")
    if pred_full == pred_lora:
        print(f"Agreement: {label_names[pred_full].upper()}")
    else:
        print("Disagreement!")

# Test examples
examples = [
    "I'm so excited about this!",
    "I feel really sad and alone",
    "This makes me so angry!"
]

print("Example predictions:")
for ex in examples:
    predict_emotion(ex)

print("\nYour turn! Type sentences to classify:")
while True:
    text = input("\nSentence: ").strip()
    if not text:
        break
    predict_emotion(text)

print("\n" + "="*70)
print("Demo complete!")
print("="*70)