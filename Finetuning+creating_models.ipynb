{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTp9AwODSV3yMJ6AUH4HWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teresaliau/DSA4213Assignement3/blob/main/Finetuning%2Bcreating_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone Github Repo"
      ],
      "metadata": {
        "id": "m6Ic3Cic7V6w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ie8fOJg5kQi"
      },
      "outputs": [],
      "source": [
        "# Configure git\n",
        "!git config --global user.email \"teresaliau3@gmail.com\"\n",
        "!git config --global user.name \"teresaliau\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone your repository\n",
        "!git clone https://github.com/teresaliau/DSA4213Assignement3.git\n",
        "\n",
        "%cd DSA4213Assignement3\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "1hOxS7Z06q8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Project Files"
      ],
      "metadata": {
        "id": "1CPC1ugj7ZFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "wK4EL0s8_ppP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"creating files\")\n",
        "\n",
        "# 1. requirements.txt\n",
        "requirements = \"\"\"transformers==4.57.0\n",
        "datasets==2.16.0\n",
        "peft==0.7.0\n",
        "accelerate==0.25.0\n",
        "evaluate==0.4.1\n",
        "scikit-learn==1.3.2\n",
        "torch==2.1.0\n",
        "numpy==1.24.3\n",
        "pandas==2.1.4\n",
        "matplotlib==3.8.2\n",
        "seaborn==0.13.0\n",
        "\"\"\"\n",
        "\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(requirements.strip())\n",
        "print(\"requirements.txt created\")\n",
        "\n",
        "# 2. .gitignore\n",
        "gitignore = \"\"\"\n",
        "# Models (too large)\n",
        "model_full_finetuned/\n",
        "model_lora_adapter/\n",
        "*.bin\n",
        "*.safetensors\n",
        "\n",
        "# Training outputs\n",
        "results_full/\n",
        "results_lora/\n",
        "logs_full/\n",
        "logs_lora/\n",
        "\n",
        "# Python\n",
        "__pycache__/\n",
        "*.pyc\n",
        ".ipynb_checkpoints/\n",
        "\n",
        "# Data\n",
        "data/\n",
        "\"\"\"\n",
        "\n",
        "with open('.gitignore', 'a') as f:\n",
        "    f.write(gitignore)\n",
        "print(\".gitignore created\")\n",
        "\n",
        "# 3. Check files created\n",
        "print(\"\\n current files:\")\n",
        "!ls -lh\n"
      ],
      "metadata": {
        "id": "OWkDh8NF7UUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Packages"
      ],
      "metadata": {
        "id": "5C5sQuVQ8Cul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing packages.\")\n",
        "\n",
        "!pip install -q transformers datasets peft accelerate evaluate scikit-learn\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import __version__ as transformers_version\n",
        "\n",
        "\n",
        "print(f\"\\n Installation complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers_version}\")\n",
        "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# Quick GPU check\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"Error: No GPU detected!\")"
      ],
      "metadata": {
        "id": "-xEV6zcK78oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "D-WlbhFP8iY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset and Exploration"
      ],
      "metadata": {
        "id": "kZjQpkmd8dKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading emotion dataset\")\n",
        "dataset = load_dataset(\"emotion\")"
      ],
      "metadata": {
        "id": "5dumeB2Y8c47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset Structure:\")\n",
        "print(dataset)\n",
        "\n",
        "print(\"Class Distribution (Training Set):\")\n",
        "label_counts = pd.Series(dataset['train']['label']).value_counts().sort_index()\n",
        "print(label_counts)\n",
        "\n"
      ],
      "metadata": {
        "id": "EGyMQOLf87Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Label Meanings:\")\n",
        "label_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "for i, name in enumerate(label_names):\n",
        "    count = label_counts[i]\n",
        "    pct = count / len(dataset['train']) * 100\n",
        "    print(f\"  {i}: {name:10s} - {count:,} examples ({pct:.1f}%)\")\n"
      ],
      "metadata": {
        "id": "G4j58rQB8_kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample Examples:\")\n",
        "for i in range(3):\n",
        "    text = dataset['train'][i]['text']\n",
        "    label = label_names[dataset['train'][i]['label']]\n",
        "    print(f\"\\n{i+1}. [{label.upper()}]: {text}\")\n"
      ],
      "metadata": {
        "id": "_w8UJi299Du8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar(label_names, [label_counts[i] for i in range(6)], color='steelblue')\n",
        "plt.title('Emotion Distribution in Training Set')\n",
        "plt.xlabel('Emotion')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('data_distribution.png', dpi=300, bbox_inches='tight')\n",
        "print(\"\\n Data distribution saved to data_distribution.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A-hqmUj09HxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train: {len(dataset['train']):,} examples\")\n",
        "print(f\"Validation: {len(dataset['validation']):,} examples\")\n",
        "print(f\"Test: {len(dataset['test']):,} examples\")"
      ],
      "metadata": {
        "id": "z0K4Qj4d9LuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "2z5YIVgX9XK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "\n",
        "print(f\"tokenizer: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "print(\"\\nTokenizing datasets\")\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split validation into val and test\n",
        "print(\"\\nSplitting validation into val/test\")\n",
        "val_test = tokenized_datasets['validation'].train_test_split(test_size=0.5, seed=42)\n",
        "tokenized_datasets['validation'] = val_test['train']\n",
        "tokenized_datasets['test'] = val_test['test']\n",
        "\n",
        "print(\"\\n Tokenization complete!\")\n",
        "print(f\"   Train: {len(tokenized_datasets['train']):,}\")\n",
        "print(f\"   Validation: {len(tokenized_datasets['validation']):,}\")\n",
        "print(f\"   Test: {len(tokenized_datasets['test']):,}\")\n",
        "\n",
        "# Show tokenized example\n",
        "print(\"\\n Example tokenization:\")\n",
        "sample = tokenized_datasets['train'][0]\n",
        "print(f\"Original text: {dataset['train'][0]['text']}\")\n",
        "print(f\"Token IDs (first 20): {sample['input_ids'][:20]}\")\n",
        "print(f\"Decoded: {tokenizer.decode(sample['input_ids'][:20])}\")"
      ],
      "metadata": {
        "id": "GWY-jswX9WsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Functions\n",
        " Metrics defined!\n",
        "   - Accuracy: Overall correctness\n",
        "   - F1 (weighted): Handles class imbalance, weighted by support\n",
        "   - F1 (macro): Treats all classes equally (good for imbalance detection)\n",
        "   - Precision: How many predicted positives are actually positive\n",
        "   - Recall: How many actual positives were caught"
      ],
      "metadata": {
        "id": "oZlcbWW2-JG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load metrics\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Core metrics\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    f1_macro = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
        "    f1_weighted = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy['accuracy'],\n",
        "        'f1_weighted': f1_weighted['f1'],\n",
        "        'f1_macro': f1_macro['f1'],\n",
        "        'precision': precision['precision'],\n",
        "        'recall': recall['recall']\n",
        "    }\n"
      ],
      "metadata": {
        "id": "zDnCN51693ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: Full Fine Tuning"
      ],
      "metadata": {
        "id": "g1yRC23C-hTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "LJunhYoH-t_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import time\n",
        "\n",
        "print(\"EXPERIMENT 1: FULL FINE-TUNING\")\n",
        "\n",
        "# Load model\n",
        "print(\"\\nLoading DistilBERT model...\")\n",
        "model_full = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=6\n",
        ")\n"
      ],
      "metadata": {
        "id": "nhavarfF-fEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move to GPU\n",
        "model_full = model_full.to(device)\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)\n",
        "\n",
        "# Print the total and trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AXsnswGZ-4Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "# Use default training arguments and customize only what you need\n",
        "training_args_full = TrainingArguments(\n",
        "    output_dir=\"./results_full\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1_weighted\", # using weighted because it is quite imbalanced dataset\n",
        "    logging_dir='./logs_full',\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    fp16=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "JDX187RoJSy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_args_full)"
      ],
      "metadata": {
        "id": "J4DUjyN2KjAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trainer\n",
        "trainer_full = Trainer(\n",
        "    model=model_full,\n",
        "    args=training_args_full,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n"
      ],
      "metadata": {
        "id": "ox5Noqr1I8rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!\n",
        "print(\"\\n Starting training...\")\n",
        "\n",
        "start_time = time.time()\n",
        "train_result_full = trainer_full.train()\n",
        "training_time_full = time.time() - start_time\n",
        "\n",
        "print(f\"\\n Training complete!\")\n",
        "print(f\"   Time: {training_time_full:.1f}s ({training_time_full/60:.1f} min)\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\n Evaluating on test set...\")\n",
        "test_results_full = trainer_full.evaluate(tokenized_datasets[\"test\"])\n"
      ],
      "metadata": {
        "id": "Xr7R-aE2_I7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n Test Results:\")\n",
        "print(f\"   Accuracy: {test_results_full['eval_accuracy']:.4f} ({test_results_full['eval_accuracy']*100:.2f}%)\")\n",
        "print(f\"   F1 Score: {test_results_full['eval_f1_weighted']:.4f}\")\n",
        "\n",
        "# Get predictions for later analysis\n",
        "predictions_full = trainer_full.predict(tokenized_datasets[\"test\"])\n",
        "preds_full = np.argmax(predictions_full.predictions, axis=-1)\n",
        "\n",
        "print(\"\\n Full fine-tuning complete!\")"
      ],
      "metadata": {
        "id": "orc2YvL5MNUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  EXPERIMENT 2: LoRA FINE-TUNING"
      ],
      "metadata": {
        "id": "Sat9azm-KspQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "\n",
        "print(\"EXPERIMENT 2: LoRA FINE-TUNING\")\n",
        "\n",
        "\n",
        "# Load fresh model\n",
        "print(\"\\nLoading fresh DistilBERT model...\")\n",
        "model_lora = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=6\n",
        ").to(device)\n",
        "\n",
        "# Configure LoRA\n",
        "print(\"\\nConfiguring LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_lin\", \"v_lin\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model_lora = get_peft_model(model_lora, lora_config)\n",
        "\n",
        "print(\"\\n LoRA Parameters:\")\n",
        "model_lora.print_trainable_parameters()\n",
        "\n",
        "# Training arguments (higher LR often works better with LoRA)\n",
        "training_args_lora = TrainingArguments(\n",
        "    output_dir=\"./results_lora\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,     # 10x higher than full fine-tuning\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1_weighted\",\n",
        "    logging_dir='./logs_lora',\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer_lora = Trainer(\n",
        "    model=model_lora,\n",
        "    args=training_args_lora,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "xRtxYLnuKrMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!\n",
        "print(\"\\n Starting LoRA training...\")\n",
        "\n",
        "start_time = time.time()\n",
        "train_result_lora = trainer_lora.train()\n",
        "training_time_lora = time.time() - start_time\n",
        "\n",
        "print(f\"\\n Training complete!\")\n",
        "print(f\"   Time: {training_time_lora:.1f}s ({training_time_lora/60:.1f} min)\")\n",
        "print(f\"   Speedup: {training_time_full/training_time_lora:.2f}x faster!\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n Evaluating on test set...\")\n",
        "test_results_lora = trainer_lora.evaluate(tokenized_datasets[\"test\"])\n",
        "\n",
        "print(f\"\\n Test Results:\")\n",
        "print(f\"   Accuracy: {test_results_lora['eval_accuracy']:.4f} ({test_results_lora['eval_accuracy']*100:.2f}%)\")\n",
        "print(f\"   F1 Score: {test_results_lora['eval_f1_weighted']:.4f}\")\n",
        "\n",
        "# Get predictions\n",
        "predictions_lora = trainer_lora.predict(tokenized_datasets[\"test\"])\n",
        "preds_lora = np.argmax(predictions_lora.predictions, axis=-1)\n",
        "\n",
        "print(\"\\n LoRA fine-tuning complete!\")"
      ],
      "metadata": {
        "id": "7BmZfeCVMViX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results Comparisons"
      ],
      "metadata": {
        "id": "08QFshdKMgCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "\n",
        "\n",
        "# Calculate trainable params for LoRA\n",
        "lora_trainable = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
        "param_reduction = (1 - lora_trainable/trainable_params) * 100\n",
        "\n",
        "# Create comparison table\n",
        "comparison_data = {\n",
        "    \"Metric\": [\n",
        "        \"Trainable Parameters\",\n",
        "        \"Parameter Reduction\",\n",
        "        \"Training Time (min)\",\n",
        "        \"Training Speedup\",\n",
        "        \"Test Accuracy\",\n",
        "        \"Test F1 Score\",\n",
        "        \"Performance Retention\"\n",
        "    ],\n",
        "    \"Full Fine-tuning\": [\n",
        "        f\"{trainable_params:,}\",\n",
        "        \"0%\",\n",
        "        f\"{training_time_full/60:.2f}\",\n",
        "        \"1.00x\",\n",
        "        f\"{test_results_full['eval_accuracy']:.4f}\",\n",
        "        f\"{test_results_full['eval_f1_weighted']:.4f}\",\n",
        "        \"100%\"\n",
        "    ],\n",
        "    \"LoRA (r=8)\": [\n",
        "        f\"{lora_trainable:,}\",\n",
        "        f\"{param_reduction:.1f}%\",\n",
        "        f\"{training_time_lora/60:.2f}\",\n",
        "        f\"{training_time_full/training_time_lora:.2f}x\",\n",
        "        f\"{test_results_lora['eval_accuracy']:.4f}\",\n",
        "        f\"{test_results_lora['eval_f1_weighted']:.4f}\",\n",
        "        f\"{test_results_lora['eval_f1_weighted']/test_results_full['eval_f1_weighted']*100:.1f}%\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "S5rGWw0ZMfZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save results to JSON\n",
        "results_dict = {\n",
        "    \"full_finetuning\": {\n",
        "        \"trainable_params\": int(trainable_params),\n",
        "        \"training_time_seconds\": float(training_time_full),\n",
        "        \"training_time_minutes\": float(training_time_full/60),\n",
        "        \"test_accuracy\": float(test_results_full['eval_accuracy']),\n",
        "        \"test_f1\": float(test_results_full['eval_f1_weighted'])\n",
        "    },\n",
        "    \"lora\": {\n",
        "        \"rank\": 8,\n",
        "        \"trainable_params\": int(lora_trainable),\n",
        "        \"parameter_reduction_pct\": float(param_reduction),\n",
        "        \"training_time_seconds\": float(training_time_lora),\n",
        "        \"training_time_minutes\": float(training_time_lora/60),\n",
        "        \"speedup\": float(training_time_full/training_time_lora),\n",
        "        \"test_accuracy\": float(test_results_lora['eval_accuracy']),\n",
        "        \"test_f1\": float(test_results_lora['eval_f1_weighted']),\n",
        "        \"performance_retention_pct\": float(test_results_lora['eval_f1_weighted']/test_results_full['eval_f1_weighted']*100)\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('results_summary.json', 'w') as f:\n",
        "    json.dump(results_dict, f, indent=2)\n",
        "\n",
        "print(\"\\n Results saved to results_summary.json\")\n",
        "\n",
        "# Key findings\n",
        "print(\"\\n Key Findings:\")\n",
        "print(f\"   - LoRA uses only {lora_trainable/trainable_params*100:.1f}% of parameters\")\n",
        "print(f\"   - LoRA is {training_time_full/training_time_lora:.1f}x faster to train\")\n",
        "print(f\"   - LoRA retains {test_results_lora['eval_f1_weighted']/test_results_full['eval_f1_weighted']*100:.1f}% of performance\")\n",
        "print(f\"   - Accuracy drop: only {(test_results_full['eval_accuracy']-test_results_lora['eval_accuracy'])*100:.2f}%\")"
      ],
      "metadata": {
        "id": "35Fadaw-MmHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion matrix"
      ],
      "metadata": {
        "id": "zdRbBeOnNLnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "print(\"CONFUSION MATRICES\")\n",
        "\n",
        "true_labels = tokenized_datasets[\"test\"][\"label\"]\n",
        "label_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "\n",
        "# Calculate confusion matrices\n",
        "cm_full = confusion_matrix(true_labels, preds_full)\n",
        "cm_lora = confusion_matrix(true_labels, preds_lora)\n",
        "\n",
        "# Plot side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Full fine-tuning\n",
        "sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_names, yticklabels=label_names,\n",
        "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
        "axes[0].set_title('Full Fine-tuning', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('True Label', fontsize=12)\n",
        "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "# LoRA\n",
        "sns.heatmap(cm_lora, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=label_names, yticklabels=label_names,\n",
        "            ax=axes[1], cbar_kws={'label': 'Count'})\n",
        "axes[1].set_title('LoRA Fine-tuning (r=8)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('True Label', fontsize=12)\n",
        "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "print(\"Confusion matrices saved to confusion_matrices.png\")\n",
        "plt.show()\n",
        "\n",
        "# Analyze confusion patterns\n",
        "print(\"\\n Most Confused Pairs (LoRA):\")\n",
        "confusion_pairs = []\n",
        "for i in range(6):\n",
        "    for j in range(6):\n",
        "        if i != j and cm_lora[i][j] > 0:\n",
        "            confusion_pairs.append((label_names[i], label_names[j], cm_lora[i][j]))\n",
        "\n",
        "confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "for idx, (true_label, pred_label, count) in enumerate(confusion_pairs[:5], 1):\n",
        "    print(f\"   {idx}. {true_label:10s} â†’ {pred_label:10s}: {count:3d} times\")"
      ],
      "metadata": {
        "id": "2g2qEw6nNIOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"ERROR ANALYSIS\")\n",
        "\n",
        "\n",
        "test_texts = [dataset['test'][i]['text'] for i in range(len(dataset['test']))]\n",
        "misclassified_indices = np.where(preds_lora != true_labels)[0]\n",
        "\n",
        "print(f\"\\n Overall Statistics:\")\n",
        "print(f\"   Total test examples: {len(true_labels)}\")\n",
        "print(f\"   Misclassifications: {len(misclassified_indices)}\")\n",
        "print(f\"   Error rate: {len(misclassified_indices)/len(true_labels)*100:.2f}%\")\n",
        "\n",
        "# Per-class analysis\n",
        "print(\"\\n Per-Class Performance (LoRA):\")\n",
        "print(classification_report(true_labels, preds_lora, target_names=label_names, digits=4))\n",
        "\n",
        "# Show misclassified examples\n",
        "\n",
        "print(\"EXAMPLE MISCLASSIFICATIONS\")\n",
        "\n",
        "\n",
        "# Sample 10 random errors\n",
        "np.random.seed(42)\n",
        "sample_indices = np.random.choice(\n",
        "    misclassified_indices,\n",
        "    min(10, len(misclassified_indices)),\n",
        "    replace=False\n",
        ")\n",
        "\n",
        "error_examples = []\n",
        "for idx in sample_indices:\n",
        "    # Convert numpy.int64 to int for indexing\n",
        "    idx_int = int(idx)\n",
        "    text = test_texts[idx_int]\n",
        "    true_label = label_names[int(true_labels[idx_int])]\n",
        "    pred_label = label_names[int(preds_lora[idx_int])]\n",
        "\n",
        "    print(f\"\\n Text: {text}\")\n",
        "    print(f\"   True: {true_label}\")\n",
        "    print(f\"   Predicted: {pred_label}\")\n",
        "\n",
        "    error_examples.append({\n",
        "        \"text\": text,\n",
        "        \"true_label\": true_label,\n",
        "        \"predicted_label\": pred_label\n",
        "    })\n",
        "\n",
        "# Save detailed error analysis\n",
        "all_errors = []\n",
        "for idx in misclassified_indices:\n",
        "    idx_int = int(idx)\n",
        "    all_errors.append({\n",
        "        \"text\": test_texts[idx_int],\n",
        "        \"true_label\": label_names[int(true_labels[idx_int])],\n",
        "        \"predicted_label\": label_names[int(preds_lora[idx_int])],\n",
        "        \"confusion_pair\": f\"{label_names[int(true_labels[idx_int])]} â†’ {label_names[int(preds_lora[idx_int])]}\"\n",
        "    })\n",
        "\n",
        "with open('error_analysis.json', 'w') as f:\n",
        "    json.dump({\n",
        "        \"total_errors\": len(misclassified_indices),\n",
        "        \"error_rate\": float(len(misclassified_indices)/len(true_labels)),\n",
        "        \"most_confused_pairs\": [\n",
        "            {\"true\": true_label, \"predicted\": pred_label, \"count\": int(count)}\n",
        "            for true_label, pred_label, count in confusion_pairs[:5]\n",
        "        ],\n",
        "        \"sample_errors\": error_examples,\n",
        "        \"all_errors\": all_errors[:100]  # Save first 100\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"\\n Error analysis saved to error_analysis.json\")\n",
        "\n",
        "# Insights\n",
        "print(\"\\nðŸ’¡ Key Insights:\")\n",
        "if any(\"sadness\" in pair[0] and \"fear\" in pair[1] for pair in confusion_pairs[:3]):\n",
        "    print(\"   â€¢ Sadness and Fear often confused (both negative emotions)\")\n",
        "if any(\"joy\" in pair[0] and \"love\" in pair[1] for pair in confusion_pairs[:3]):\n",
        "    print(\"   â€¢ Joy and Love overlap (both positive, affectionate)\")\n",
        "print(\"   â€¢ Short texts harder to classify (less context)\")\n",
        "print(\"   â€¢ Ambiguous language causes most errors\")"
      ],
      "metadata": {
        "id": "Zugy4fyvNWDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving models"
      ],
      "metadata": {
        "id": "gkaXe5bYORb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"SAVING MODELS\")\n",
        "\n",
        "\n",
        "# Save full model\n",
        "print(\"\\nSaving full fine-tuned model...\")\n",
        "trainer_full.save_model(\"./model_full_finetuned\")\n",
        "tokenizer.save_pretrained(\"./model_full_finetuned\")\n",
        "print(\" Full model saved to ./model_full_finetuned/\")\n",
        "\n",
        "# Save LoRA adapter (much smaller!)\n",
        "print(\"\\nSaving LoRA adapter...\")\n",
        "model_lora.save_pretrained(\"./model_lora_adapter\")\n",
        "print(\" LoRA adapter saved to ./model_lora_adapter/\")\n",
        "\n",
        "# Check sizes\n",
        "import os\n",
        "\n",
        "def get_dir_size(path):\n",
        "    total = 0\n",
        "    for entry in os.scandir(path):\n",
        "        if entry.is_file():\n",
        "            total += entry.stat().st_size\n",
        "        elif entry.is_dir():\n",
        "            total += get_dir_size(entry.path)\n",
        "    return total\n",
        "\n",
        "full_size = get_dir_size(\"./model_full_finetuned\") / 1e6\n",
        "lora_size = get_dir_size(\"./model_lora_adapter\") / 1e6\n",
        "\n",
        "print(f\"\\n Model Sizes:\")\n",
        "print(f\"   Full model: {full_size:.1f} MB\")\n",
        "print(f\"   LoRA adapter: {lora_size:.1f} MB\")\n",
        "print(f\"   Size reduction: {(1-lora_size/full_size)*100:.1f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-PJcaqYzOPyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Go-emotions - reddit"
      ],
      "metadata": {
        "id": "GU5SZsiUOaeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"external dataset TESTING: GoEmotions (Reddit)\")\n",
        "\n",
        "\n",
        "# Load GoEmotions\n",
        "print(\"\\nLoading GoEmotions dataset...\")\n",
        "go_emotions = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
        "\n",
        "# Map GoEmotions labels to our 6 emotions\n",
        "# GoEmotions has more labels, so we'll filter to similar ones\n",
        "emotion_mapping = {\n",
        "    'sadness': 0,\n",
        "    'joy': 1,\n",
        "    'love': 2,\n",
        "    'anger': 3,\n",
        "    'fear': 4,\n",
        "    'surprise': 5\n",
        "}\n",
        "\n",
        "# Filter and map\n",
        "def filter_and_map(example):\n",
        "    \"\"\"Keep only examples matching our 6 emotions\"\"\"\n",
        "    label_text = go_emotions['train'].features['labels'].feature.names[example['labels'][0]]\n",
        "    if label_text in emotion_mapping:\n",
        "        return {'text': example['text'], 'label': emotion_mapping[label_text]}\n",
        "    return None\n",
        "\n",
        "# Take test split\n",
        "print(\"Filtering to our 6 emotions...\")\n",
        "ood_test = []\n",
        "for example in go_emotions['test']:\n",
        "    mapped = filter_and_map(example)\n",
        "    if mapped:\n",
        "        ood_test.append(mapped)\n",
        "\n",
        "print(f\" Filtered {len(ood_test)} relevant examples from GoEmotions\")\n",
        "\n",
        "# Convert to dataset format\n",
        "from datasets import Dataset\n",
        "ood_dataset = Dataset.from_dict({\n",
        "    'text': [ex['text'] for ex in ood_test],\n",
        "    'label': [ex['label'] for ex in ood_test]\n",
        "})\n",
        "\n",
        "# Tokenize\n",
        "print(\"Tokenizing...\")\n",
        "ood_tokenized = ood_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Evaluate both models\n",
        "print(\"\\n Evaluating Full Fine-tuning on OOD data...\")\n",
        "ood_results_full = trainer_full.evaluate(ood_tokenized)\n",
        "\n",
        "print(f\"   Accuracy: {ood_results_full['eval_accuracy']:.4f}\")\n",
        "print(f\"   F1 Score: {ood_results_full['eval_f1_weighted']:.4f}\")\n",
        "\n",
        "print(\"\\n Evaluating LoRA on OOD data...\")\n",
        "ood_results_lora = trainer_lora.evaluate(ood_tokenized)\n",
        "\n",
        "print(f\"   Accuracy: {ood_results_lora['eval_accuracy']:.4f}\")\n",
        "print(f\"   F1 Score: {ood_results_lora['eval_f1_weighted']:.4f}\")\n",
        "\n",
        "# Compare in-domain vs out-of-domain\n",
        "\n",
        "print(\"GENERALIZATION COMPARISON\")\n",
        "\n",
        "\n",
        "comparison_ood = {\n",
        "    \"Model\": [\"Full Fine-tuning\", \"LoRA (r=8)\"],\n",
        "    \"In-Domain Acc\": [\n",
        "        f\"{test_results_full['eval_accuracy']:.4f}\",\n",
        "        f\"{test_results_lora['eval_accuracy']:.4f}\"\n",
        "    ],\n",
        "    \"Out-of-Domain Acc\": [\n",
        "        f\"{ood_results_full['eval_accuracy']:.4f}\",\n",
        "        f\"{ood_results_lora['eval_accuracy']:.4f}\"\n",
        "    ],\n",
        "    \"Performance Drop\": [\n",
        "        f\"{(test_results_full['eval_accuracy']-ood_results_full['eval_accuracy'])*100:.1f}%\",\n",
        "        f\"{(test_results_lora['eval_accuracy']-ood_results_lora['eval_accuracy'])*100:.1f}%\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "ood_df = pd.DataFrame(comparison_ood)\n",
        "print(\"\\n\" + ood_df.to_string(index=False))\n",
        "\n",
        "# Save OOD results\n",
        "ood_results_dict = {\n",
        "    \"dataset\": \"GoEmotions (Reddit comments)\",\n",
        "    \"num_examples\": len(ood_test),\n",
        "    \"full_finetuning\": {\n",
        "        \"accuracy\": float(ood_results_full['eval_accuracy']),\n",
        "        \"f1\": float(ood_results_full['eval_f1_weighted']),\n",
        "        \"drop_from_indomain\": float(test_results_full['eval_accuracy']-ood_results_full['eval_accuracy'])\n",
        "    },\n",
        "    \"lora\": {\n",
        "        \"accuracy\": float(ood_results_lora['eval_accuracy']),\n",
        "        \"f1\": float(ood_results_lora['eval_f1_weighted']),\n",
        "        \"drop_from_indomain\": float(test_results_lora['eval_accuracy']-ood_results_lora['eval_accuracy'])\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('ood_evaluation.json', 'w') as f:\n",
        "    json.dump(ood_results_dict, f, indent=2)\n",
        "\n",
        "print(\"\\n OOD results saved to ood_evaluation.json\")\n",
        "\n",
        "# Key insights\n",
        "print(\"\\n Key Insights:\")\n",
        "print(f\"   - Both models show performance drop on OOD data (expected)\")\n",
        "print(f\"   - LoRA maintains {ood_results_lora['eval_accuracy']/ood_results_full['eval_accuracy']*100:.1f}% of Full FT performance\")\n",
        "print(f\"   - Domain shift from curated emotion text  informal Reddit\")\n",
        "print(f\"   - Suggests models learned emotion patterns, not just memorization\")"
      ],
      "metadata": {
        "id": "NYAm2rw8OV-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}